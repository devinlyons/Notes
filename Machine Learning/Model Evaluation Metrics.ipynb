{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table containing *true positives*, *true negatives*, *false positives*, and *false negatives*.\n",
    "\n",
    "|Actual|Guessed Positive|Guessed Negative|\n",
    "|------|----------------|----------------|\n",
    "|**Positive**|True Positives|False Negatives|\n",
    "|**Negative**|False Positives|True Negatives|\n",
    "\n",
    "A *False Positive* is also called a *Type 1 Error*.  \n",
    "A *False Negative* is also called a *Type 2 Error*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "\n",
    "Accuracy is the ratio of accurate predictions to the total number of predictions.\n",
    "Accuracy dose not work well when the purpose of the model is to identify outliers.\n",
    "An example of this is credit car fraud.\n",
    "If only 1% of transactions are fraudulent, then predicting all transaction are legitimate has an accuracy of 99% to spite the fact that the model has failed to achieve it's purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and Recall\n",
    "Precision and Recall work when the purpose of a model is to eliminate type 1 or 2 errors.\n",
    "Recall works well to reduce type 2 errors. There are called *High Recall* models.\n",
    "Precision works well to reduce type 1 errors. There are called *High Precision* models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision\n",
    "Precision is the ratio of *true positive* by the sum of *true positives* and *false positives*.\n",
    "A model with high precision will have fewer type 1 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall\n",
    "Recall is the ratio of *true positives* by the sum of *true positives* and *false negatives*.\n",
    "A model with high recall will have fewer type 2 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Score\n",
    "The F1 Score combines precision and recall into a single value using the harmonic mean.\n",
    "This formula results in a very small number if either number is small but will still result the the same value if both numbers are equal.\n",
    "This means that if precision and recall are high, the F1 score will be high.\n",
    "However, if precision or recall are low, the F1 score will be low.\n",
    "\n",
    "$F1 Score = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F-beta Score\n",
    "F-beta Score is a more general version of the F1 Score.\n",
    "This works by choosing a beta that will skew the score.\n",
    "A beta in the range $[0,1)$ will skew the score toward precision.\n",
    "This means that precision will have a greater impact on the score than recall.\n",
    "A beta greater than 1 will skew the score toward recall.\n",
    "\n",
    "$F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Receiver Operating Characteristic(ROC) Curve\n",
    "The ROC Curve works by attempting to classify data according to some variable.\n",
    "For each value of that variable that results in a change in the number of correctly and incorrectly classified data, 2 values are calculated.\n",
    "\n",
    "$True Positive Rate = \\frac{True Positives}{All Positives}$\n",
    "\n",
    "$False Positive Rate = \\frac{False Positives}{All Negatives}$\n",
    "\n",
    "The these values create points in the form of $(False Positive Rate, True Positive Rate)$.\n",
    "These points are plotted on a graph and should result in a curve.\n",
    "The area under the curve specifies the accuracy of the model with a value in the range $(0, 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Metrics\n",
    "\n",
    "### Mean Absolute Error\n",
    "This is calculated by taking the average of the error between a models prediction and the actual value.\n",
    "\n",
    "### Mean Squared Error(MSE)\n",
    "This is calculated by taking the average of the error squared between a models prediction and the actual value.\n",
    "\n",
    "### R2 Score\n",
    "This is calculated by comparing the $MSE$ of a model ($M_n$) to the $MSE$ of the simplest possible model ($M_0$).\n",
    "A simple model is typically something like the average of all values.\n",
    "\n",
    "$R2 = 1 - \\frac{M_n}{M_0}$\n",
    "\n",
    "The R2 Score will be in the range $(0, 1)$ where values closer to 1 represent a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
